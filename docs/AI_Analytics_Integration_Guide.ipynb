{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9b1f4f",
   "metadata": {},
   "source": [
    "# ü§ñ AI Analytics Integration Guide for DV Processing System\n",
    "\n",
    "This comprehensive guide covers the recommended AI/API services for analyzing your disbursement voucher (DV) processing workflow, identifying bottlenecks, and providing actionable insights to improve efficiency.\n",
    "\n",
    "## üéØ **Goals of AI Integration:**\n",
    "- **Process Time Analysis**: Identify where DVs get stuck\n",
    "- **Pattern Recognition**: Detect seasonal or departmental trends\n",
    "- **Bottleneck Detection**: Find workflow inefficiencies\n",
    "- **Predictive Analytics**: Forecast processing times\n",
    "- **Anomaly Detection**: Flag unusual processing delays\n",
    "- **Performance Optimization**: Suggest workflow improvements\n",
    "\n",
    "---\n",
    "\n",
    "## üìã **Recommended AI Services Overview**\n",
    "\n",
    "| Service | Best For | Cost | Complexity | Government Use |\n",
    "|---------|----------|------|------------|----------------|\n",
    "| **OpenAI GPT-4** | Text analysis, insights generation | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ |\n",
    "| **Google Gemini** | Multi-modal analysis, cost-effective | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ |\n",
    "| **Azure OpenAI** | Enterprise security, government compliance | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "| **Claude (Anthropic)** | Data analysis, reasoning | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ |\n",
    "| **Local AI (Ollama)** | Privacy, no external data sharing | ‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n",
    "\n",
    "**üìù Note**: For government agencies, Azure OpenAI and Local AI solutions are recommended for data security compliance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf77d4",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Select an API/AI Service for Analysis**\n",
    "\n",
    "### **üèÜ Top Recommendation: Azure OpenAI Service**\n",
    "**Perfect for government agencies like DA-CAR**\n",
    "\n",
    "**Why Azure OpenAI?**\n",
    "- ‚úÖ **Government Compliance**: SOC 2, FedRAMP certified\n",
    "- ‚úÖ **Data Security**: Data stays in your Azure region\n",
    "- ‚úÖ **Enterprise Features**: Advanced security, monitoring\n",
    "- ‚úÖ **Multiple Models**: GPT-4, GPT-3.5, Embeddings\n",
    "- ‚úÖ **Philippines Support**: Available in Southeast Asia region\n",
    "\n",
    "### **ü•à Alternative Options:**\n",
    "\n",
    "#### **Option A: OpenAI API (Direct)**\n",
    "- **Pros**: Latest models, easy integration, extensive documentation\n",
    "- **Cons**: Data sent to external servers, higher privacy concerns\n",
    "- **Best for**: Non-sensitive analysis, prototyping\n",
    "\n",
    "#### **Option B: Google Gemini API**\n",
    "- **Pros**: Cost-effective, multimodal capabilities, good performance\n",
    "- **Cons**: Google's data handling policies\n",
    "- **Best for**: Budget-conscious projects, image + text analysis\n",
    "\n",
    "#### **Option C: Local AI (Ollama + Llama 3)**\n",
    "- **Pros**: Complete data privacy, no recurring costs, full control\n",
    "- **Cons**: Requires powerful hardware, complex setup\n",
    "- **Best for**: Highly sensitive data, offline operations\n",
    "\n",
    "#### **Option D: Claude (Anthropic)**\n",
    "- **Pros**: Excellent reasoning, good for analysis, safety-focused\n",
    "- **Cons**: API access limitations, newer service\n",
    "- **Best for**: Complex data analysis, research tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ab803",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Set Up API Access (Obtain Keys/Tokens)**\n",
    "\n",
    "### **üîê Azure OpenAI Setup (Recommended)**\n",
    "\n",
    "#### **Step 1: Create Azure Account**\n",
    "1. Go to [Azure Portal](https://portal.azure.com)\n",
    "2. Sign up with your government email\n",
    "3. Choose **Pay-as-you-go** or request organizational billing\n",
    "\n",
    "#### **Step 2: Request Azure OpenAI Access**\n",
    "1. Visit [Azure OpenAI Form](https://aka.ms/oai/access)\n",
    "2. Fill out the application form\n",
    "3. **Important**: Mention government use case for faster approval\n",
    "4. Wait for approval (usually 1-2 business days)\n",
    "\n",
    "#### **Step 3: Create Azure OpenAI Resource**\n",
    "1. In Azure Portal, click **\"Create a resource\"**\n",
    "2. Search for **\"Azure OpenAI\"**\n",
    "3. Configure:\n",
    "   - **Region**: Southeast Asia (Singapore) for PH\n",
    "   - **Pricing Tier**: Standard S0\n",
    "   - **Resource Group**: Create new (e.g., \"da-car-ai\")\n",
    "\n",
    "#### **Step 4: Deploy Models**\n",
    "1. Go to your Azure OpenAI resource\n",
    "2. Click **\"Model deployments\"** > **\"Create\"**\n",
    "3. Deploy these models:\n",
    "   - **GPT-4** (for complex analysis)\n",
    "   - **GPT-3.5-turbo** (for cost-effective tasks)\n",
    "   - **text-embedding-ada-002** (for document similarity)\n",
    "\n",
    "#### **Step 5: Get API Keys**\n",
    "1. In your Azure OpenAI resource, go to **\"Keys and Endpoint\"**\n",
    "2. Copy:\n",
    "   - **Key 1** (your API key)\n",
    "   - **Endpoint** (your service URL)\n",
    "   - **Resource Name** (for deployment names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457f41e8",
   "metadata": {},
   "source": [
    "### **üîë Alternative API Setups**\n",
    "\n",
    "#### **OpenAI API (Direct)**\n",
    "1. Go to [OpenAI Platform](https://platform.openai.com)\n",
    "2. Sign up and verify your account\n",
    "3. Go to **API Keys** section\n",
    "4. Click **\"Create new secret key\"**\n",
    "5. Copy and save the key securely\n",
    "\n",
    "#### **Google Gemini API**\n",
    "1. Visit [Google AI Studio](https://makersuite.google.com)\n",
    "2. Sign in with Google account\n",
    "3. Click **\"Get API Key\"**\n",
    "4. Create new project or select existing\n",
    "5. Generate and copy API key\n",
    "\n",
    "#### **Claude API (Anthropic)**\n",
    "1. Go to [Anthropic Console](https://console.anthropic.com)\n",
    "2. Sign up and complete verification\n",
    "3. Request API access (may require waitlist)\n",
    "4. Generate API key from dashboard\n",
    "\n",
    "#### **Local AI Setup (Ollama)**\n",
    "1. Download [Ollama](https://ollama.ai)\n",
    "2. Install on your server\n",
    "3. Pull models: `ollama pull llama3` or `ollama pull codellama`\n",
    "4. No API key needed - runs locally\n",
    "\n",
    "---\n",
    "\n",
    "### **üîí Security Best Practices**\n",
    "- **Never** commit API keys to version control\n",
    "- Use environment variables for key storage\n",
    "- Rotate keys regularly (every 90 days)\n",
    "- Monitor API usage and costs\n",
    "- Implement rate limiting in your application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c2714",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Install Required Python Libraries**\n",
    "\n",
    "### **üì¶ Core Installation Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967eb622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (1.54.5)\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.23.0-py3-none-any.whl.metadata (81 kB)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity)\n",
      "  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity)\n",
      "  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
      "Collecting cffi>=1.14 (from cryptography>=2.5->azure-identity)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=2.5->azure-identity)\n",
      "  Downloading pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.2.3)\n",
      "Downloading azure_identity-1.23.0-py3-none-any.whl (186 kB)\n",
      "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n",
      "Downloading cryptography-45.0.4-cp311-abi3-win_amd64.whl (3.4 MB)\n",
      "   ---------------------------------------- 0.0/3.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.4/3.4 MB 25.2 MB/s eta 0:00:00\n",
      "Downloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
      "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: python-dotenv, PyJWT, pycparser, cffi, azure-core, cryptography, msal, msal-extensions, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 azure-core-1.34.0 azure-identity-1.23.0 cffi-1.17.1 cryptography-45.0.4 msal-1.32.3 msal-extensions-1.3.1 pycparser-2.22 python-dotenv-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (1.54.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (1.1.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.174.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-6.31.1-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: pydantic in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (2.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Downloading googleapis_common_protos-1.70.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic->google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Collecting grpcio<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio-1.73.1-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2024.8.30)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/1.3 MB 22.7 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Downloading google_api_python_client-2.174.0-py3-none-any.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 6.3/13.7 MB 32.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 35.7 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading grpcio-1.73.1-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------------------------------- - 4.2/4.3 MB 21.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed cachetools-5.5.2 google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.174.0 google-auth-2.40.3 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 googleapis-common-protos-1.70.0 grpcio-1.73.1 grpcio-status-1.71.2 httplib2-0.22.0 proto-plus-1.26.1 protobuf-5.29.5 pyasn1-0.6.1 pyasn1-modules-0.4.2 pyparsing-3.2.3 rsa-4.9.1 uritemplate-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting anthropic\n",
      "  Downloading anthropic-0.55.0-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (2.9.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anthropic) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.5.0->anthropic) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.25.0->anthropic) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1,>=0.25.0->anthropic) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic<3,>=1.9.0->anthropic) (2.23.4)\n",
      "Downloading anthropic-0.55.0-py3-none-any.whl (289 kB)\n",
      "Installing collected packages: anthropic\n",
      "Successfully installed anthropic-0.55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.2.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp312-cp312-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp312-cp312-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-11.2.1-cp312-cp312-win_amd64.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib) (3.2.3)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.44.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.0-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 7.1/11.0 MB 36.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 40.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "   ---------------------------------------- 0.0/12.7 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.6/12.7 MB 65.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.7/12.7 MB 57.2 MB/s eta 0:00:00\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 55.4 MB/s eta 0:00:00\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading plotly-6.2.0-py3-none-any.whl (9.6 MB)\n",
      "   ---------------------------------------- 0.0/9.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 9.6/9.6 MB 60.1 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.2-cp312-cp312-win_amd64.whl (223 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.2/2.2 MB 63.7 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Downloading narwhals-1.44.0-py3-none-any.whl (365 kB)\n",
      "Downloading pillow-11.2.1-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 51.4 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pillow, numpy, narwhals, kiwisolver, fonttools, cycler, plotly, pandas, contourpy, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 narwhals-1.44.0 numpy-2.3.1 pandas-2.3.0 pillow-11.2.1 plotly-6.2.0 pytz-2025.2 seaborn-0.13.2 tzdata-2025.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000001FEED156F30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /packages/1f/d9/74017c4eec7a28892d8d6e31ae9de3baef71f5a5286e74e6b7aad7f8c837/pandas-2.3.0-cp312-cp312-win_amd64.whl.metadata\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.3.0-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Downloading mysql_connector_python-9.3.0-cp312-cp312-win_amd64.whl (16.4 MB)\n",
      "   ---------------------------------------- 0.0/16.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/16.4 MB 16.9 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.8/16.4 MB 7.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.6/16.4 MB 4.2 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.6/16.4 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.2/16.4 MB 10.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.9/16.4 MB 12.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.4/16.4 MB 12.4 MB/s eta 0:00:00\n",
      "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: pymysql, mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.3.0 pymysql-1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from fastapi) (4.12.2)\n",
      "Collecting click>=7.0 (from uvicorn)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.6.2.post1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: click, uvicorn, starlette, fastapi\n",
      "Successfully installed click-8.2.1 fastapi-0.115.14 starlette-0.46.2 uvicorn-0.35.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.5.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from ollama) (2.9.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27->ollama) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.9->ollama) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\alpha\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.9->ollama) (4.12.2)\n",
      "Downloading ollama-0.5.1-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Azure OpenAI (Recommended for government use)\n",
    "%pip install openai azure-identity python-dotenv\n",
    "\n",
    "# Standard OpenAI API\n",
    "%pip install openai python-dotenv\n",
    "\n",
    "# Google Gemini\n",
    "%pip install google-generativeai\n",
    "\n",
    "# Anthropic Claude\n",
    "%pip install anthropic\n",
    "\n",
    "# Data analysis and visualization libraries\n",
    "%pip install pandas numpy matplotlib seaborn plotly\n",
    "\n",
    "# Database connectivity (for Laravel integration)\n",
    "%pip install mysql-connector-python pymysql\n",
    "\n",
    "# Web framework (for API endpoints)\n",
    "%pip install fastapi uvicorn\n",
    "\n",
    "# Local AI (Ollama)\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72b42da",
   "metadata": {},
   "source": [
    "### **üìã Create Requirements File**\n",
    "\n",
    "Create a `requirements.txt` file for your project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62516329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ requirements.txt created successfully!\n",
      "\n",
      "To install all packages, run:\n",
      "pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Create requirements.txt file\n",
    "requirements_content = \"\"\"\n",
    "# AI Services\n",
    "openai==1.30.0\n",
    "azure-identity==1.16.0\n",
    "google-generativeai==0.5.0\n",
    "anthropic==0.25.0\n",
    "ollama==0.1.8\n",
    "\n",
    "# Data Analysis\n",
    "pandas==2.2.0\n",
    "numpy==1.24.0\n",
    "matplotlib==3.8.0\n",
    "seaborn==0.13.0\n",
    "plotly==5.17.0\n",
    "\n",
    "# Database & Web\n",
    "mysql-connector-python==8.3.0\n",
    "pymysql==1.1.0\n",
    "fastapi==0.110.0\n",
    "uvicorn==0.29.0\n",
    "\n",
    "# Utilities\n",
    "python-dotenv==1.0.0\n",
    "requests==2.31.0\n",
    "python-dateutil==2.8.2\n",
    "\"\"\"\n",
    "\n",
    "# Save to file\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements_content.strip())\n",
    "\n",
    "print(\"‚úÖ requirements.txt created successfully!\")\n",
    "print(\"\\nTo install all packages, run:\")\n",
    "print(\"pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6cd4ee",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Authenticate and Connect to the API**\n",
    "\n",
    "### **üîê Environment Variables Setup**\n",
    "\n",
    "First, create a `.env` file to store your API keys securely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "972228dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ .env file created!\n",
      "üîí Remember to:\n",
      "1. Replace placeholder values with your actual API keys\n",
      "2. Add .env to your .gitignore file\n",
      "3. Never commit API keys to version control\n"
     ]
    }
   ],
   "source": [
    "# Create .env file with your API keys\n",
    "env_content = \"\"\"\n",
    "# Azure OpenAI (Recommended)\n",
    "AZURE_OPENAI_KEY=your-azure-openai-key-here\n",
    "AZURE_OPENAI_ENDPOINT=https://your-resource-name.openai.azure.com/\n",
    "AZURE_OPENAI_VERSION=2024-02-15-preview\n",
    "\n",
    "# Standard OpenAI (Alternative)\n",
    "OPENAI_API_KEY=your-openai-api-key-here\n",
    "\n",
    "# Google Gemini (Alternative)\n",
    "GOOGLE_API_KEY=your-google-api-key-here\n",
    "\n",
    "# Anthropic Claude (Alternative)\n",
    "ANTHROPIC_API_KEY=your-anthropic-api-key-here\n",
    "\n",
    "# Database Configuration (for Laravel integration)\n",
    "DB_HOST=localhost\n",
    "DB_PORT=3306\n",
    "DB_DATABASE=accounting_clean\n",
    "DB_USERNAME=your-db-username\n",
    "DB_PASSWORD=your-db-password\n",
    "\"\"\"\n",
    "\n",
    "# Save .env file\n",
    "with open('.env', 'w') as f:\n",
    "    f.write(env_content.strip())\n",
    "\n",
    "print(\"‚úÖ .env file created!\")\n",
    "print(\"üîí Remember to:\")\n",
    "print(\"1. Replace placeholder values with your actual API keys\")\n",
    "print(\"2. Add .env to your .gitignore file\")\n",
    "print(\"3. Never commit API keys to version control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3df973a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Testing API connections...\n",
      "‚úÖ Azure OpenAI client initialized successfully!\n",
      "‚úÖ OpenAI client initialized successfully!\n",
      "‚úÖ Google Gemini client initialized successfully!\n",
      "‚úÖ Google Gemini client initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpha\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Azure OpenAI Authentication (Recommended)\n",
    "def setup_azure_openai():\n",
    "    \"\"\"Setup Azure OpenAI client for government compliance\"\"\"\n",
    "    try:\n",
    "        from openai import AzureOpenAI\n",
    "        \n",
    "        client = AzureOpenAI(\n",
    "            api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "            api_version=os.getenv(\"AZURE_OPENAI_VERSION\"),\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Azure OpenAI client initialized successfully!\")\n",
    "        return client\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure OpenAI setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Standard OpenAI Authentication\n",
    "def setup_openai():\n",
    "    \"\"\"Setup standard OpenAI client\"\"\"\n",
    "    try:\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        print(\"‚úÖ OpenAI client initialized successfully!\")\n",
    "        return openai\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå OpenAI setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Google Gemini Authentication\n",
    "def setup_gemini():\n",
    "    \"\"\"Setup Google Gemini client\"\"\"\n",
    "    try:\n",
    "        import google.generativeai as genai\n",
    "        \n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        print(\"‚úÖ Google Gemini client initialized successfully!\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Google Gemini setup failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test the setup\n",
    "print(\"üîß Testing API connections...\")\n",
    "azure_client = setup_azure_openai()\n",
    "openai_client = setup_openai()\n",
    "gemini_model = setup_gemini()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ace72",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Send Data for Analysis**\n",
    "\n",
    "### **üìä Connect to Your DV Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b28e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database connection successful!\n",
      "‚ùå Data extraction failed: Execution failed on sql '\n",
      "    SELECT \n",
      "        id,\n",
      "        dv_number,\n",
      "        transaction_type,\n",
      "        payee,\n",
      "        amount,\n",
      "        particulars,\n",
      "        implementing_unit,\n",
      "        fund_source,\n",
      "        status,\n",
      "        created_at,\n",
      "        indexing_date,\n",
      "        payment_method,\n",
      "        engas_date,\n",
      "        cdj_date,\n",
      "        lddap_certification_date,\n",
      "        updated_at\n",
      "    FROM incoming_dvs \n",
      "    WHERE created_at >= DATE_SUB(NOW(), INTERVAL %s DAY)\n",
      "    ORDER BY created_at DESC\n",
      "    ': 1054 (42S22): Unknown column 'lddap_certification_date' in 'field list'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alpha\\AppData\\Local\\Temp\\ipykernel_21988\\2289514088.py:48: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection, params=[days_back])\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "def connect_to_dv_database():\n",
    "    \"\"\"Connect to your Laravel DV database\"\"\"\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=os.getenv(\"DB_HOST\", \"localhost\"),\n",
    "            port=int(os.getenv(\"DB_PORT\", 3306)),\n",
    "            database=os.getenv(\"DB_DATABASE\", \"accounting_clean\"),\n",
    "            user=os.getenv(\"DB_USERNAME\"),\n",
    "            password=os.getenv(\"DB_PASSWORD\")\n",
    "        )\n",
    "        print(\"‚úÖ Database connection successful!\")\n",
    "        return connection\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Database connection failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_dv_data(connection, days_back=90):\n",
    "    \"\"\"Extract DV data for analysis\"\"\"\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        id,\n",
    "        dv_number,\n",
    "        transaction_type,\n",
    "        payee,\n",
    "        amount,\n",
    "        particulars,\n",
    "        implementing_unit,\n",
    "        fund_source,\n",
    "        status,\n",
    "        created_at,\n",
    "        indexing_date,\n",
    "        payment_method,\n",
    "        engas_date,\n",
    "        cdj_date,\n",
    "        lddap_certification_date,\n",
    "        updated_at\n",
    "    FROM incoming_dvs \n",
    "    WHERE created_at >= DATE_SUB(NOW(), INTERVAL %s DAY)\n",
    "    ORDER BY created_at DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_sql(query, connection, params=[days_back])\n",
    "        print(f\"‚úÖ Extracted {len(df)} DV records for analysis\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data extraction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_processing_times(df):\n",
    "    \"\"\"Calculate processing times for each DV\"\"\"\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'])\n",
    "    df['updated_at'] = pd.to_datetime(df['updated_at'])\n",
    "    \n",
    "    # Calculate total processing time\n",
    "    df['total_processing_days'] = (df['updated_at'] - df['created_at']).dt.days\n",
    "    \n",
    "    # Calculate stage-specific times\n",
    "    date_columns = ['indexing_date', 'engas_date', 'cdj_date', 'lddap_certification_date']\n",
    "    for col in date_columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Connect and extract data\n",
    "connection = connect_to_dv_database()\n",
    "if connection:\n",
    "    dv_data = extract_dv_data(connection)\n",
    "    if dv_data is not None:\n",
    "        dv_data = calculate_processing_times(dv_data)\n",
    "        print(\"\\nüìä Sample data:\")\n",
    "        print(dv_data[['dv_number', 'status', 'total_processing_days']].head())\n",
    "    \n",
    "    connection.close()\n",
    "else:\n",
    "    print(\"Using sample data for demonstration...\")\n",
    "    # Create sample data if database connection fails\n",
    "    dv_data = pd.DataFrame({\n",
    "        'dv_number': ['DV-2025-001', 'DV-2025-002', 'DV-2025-003'],\n",
    "        'transaction_type': ['Professional Services', 'Supplies', 'Equipment'],\n",
    "        'payee': ['ABC Company', 'XYZ Supplier', 'Tech Solutions'],\n",
    "        'amount': [50000, 25000, 150000],\n",
    "        'status': ['processed', 'for_approval', 'for_review'],\n",
    "        'implementing_unit': ['RAED', 'SAAD', 'AMAD'],\n",
    "        'total_processing_days': [15, 8, 5]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1b0cd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dv_bottlenecks(df, ai_client):\n",
    "    \"\"\"Use AI to analyze DV processing bottlenecks\"\"\"\n",
    "    \n",
    "    # Prepare data summary for AI analysis\n",
    "    summary_stats = {\n",
    "        'total_dvs': len(df),\n",
    "        'avg_processing_days': df['total_processing_days'].mean(),\n",
    "        'status_distribution': df['status'].value_counts().to_dict(),\n",
    "        'processing_by_unit': df.groupby('implementing_unit')['total_processing_days'].mean().to_dict(),\n",
    "        'processing_by_type': df.groupby('transaction_type')['total_processing_days'].mean().to_dict(),\n",
    "        'stuck_dvs': df[df['total_processing_days'] > df['total_processing_days'].quantile(0.75)].to_dict('records')\n",
    "    }\n",
    "    \n",
    "    # Create analysis prompt\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert process analyst for a government accounting department. \n",
    "    Analyze this DV (Disbursement Voucher) processing data and provide actionable insights.\n",
    "\n",
    "    DATA SUMMARY:\n",
    "    - Total DVs processed: {summary_stats['total_dvs']}\n",
    "    - Average processing time: {summary_stats['avg_processing_days']:.1f} days\n",
    "    - Status distribution: {json.dumps(summary_stats['status_distribution'], indent=2)}\n",
    "    - Processing times by unit: {json.dumps(summary_stats['processing_by_unit'], indent=2)}\n",
    "    - Processing times by transaction type: {json.dumps(summary_stats['processing_by_type'], indent=2)}\n",
    "\n",
    "    SLOW PROCESSING DVs (top 25%):\n",
    "    {json.dumps(summary_stats['stuck_dvs'][:5], indent=2)}\n",
    "\n",
    "    Please provide:\n",
    "    1. BOTTLENECK ANALYSIS: Identify where DVs are getting stuck\n",
    "    2. EFFICIENCY RECOMMENDATIONS: Specific actions to improve processing times\n",
    "    3. UNIT PERFORMANCE: Which units need support and why\n",
    "    4. PROCESS OPTIMIZATION: Workflow changes to reduce delays\n",
    "    5. RISK FACTORS: What might cause future bottlenecks\n",
    "\n",
    "    Format your response as structured JSON with clear sections.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if hasattr(ai_client, 'chat'):  # Azure OpenAI\n",
    "            response = ai_client.chat.completions.create(\n",
    "                model=\"gpt-4\",  # or your deployed model name\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            analysis = response.choices[0].message.content\n",
    "        elif hasattr(ai_client, 'generate_content'):  # Google Gemini\n",
    "            response = ai_client.generate_content(prompt)\n",
    "            analysis = response.text\n",
    "        else:  # Standard OpenAI\n",
    "            response = ai_client.ChatCompletion.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=2000,\n",
    "                temperature=0.3\n",
    "            )\n",
    "            analysis = response.choices[0].message.content\n",
    "            \n",
    "        print(\"ü§ñ AI Analysis completed!\")\n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå AI analysis failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_processing_time(dv_details, ai_client):\n",
    "    \"\"\"Predict processing time for a new DV\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Based on historical DV processing patterns, predict the processing time for this new DV:\n",
    "    \n",
    "    DV Details:\n",
    "    - Transaction Type: {dv_details.get('transaction_type', 'Unknown')}\n",
    "    - Amount: PHP {dv_details.get('amount', 0):,.2f}\n",
    "    - Implementing Unit: {dv_details.get('implementing_unit', 'Unknown')}\n",
    "    - Fund Source: {dv_details.get('fund_source', 'Unknown')}\n",
    "    \n",
    "    Provide:\n",
    "    1. Estimated processing time (days)\n",
    "    2. Confidence level (1-10)\n",
    "    3. Potential delay factors\n",
    "    4. Recommended actions to expedite\n",
    "    \n",
    "    Respond in JSON format.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if hasattr(ai_client, 'chat'):  # Azure OpenAI\n",
    "            response = ai_client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",  # Using faster model for predictions\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=500,\n",
    "                temperature=0.2\n",
    "            )\n",
    "            prediction = response.choices[0].message.content\n",
    "        else:\n",
    "            prediction = \"Prediction service temporarily unavailable\"\n",
    "            \n",
    "        return prediction\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Prediction failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Run analysis if we have data and AI client\n",
    "if 'dv_data' in locals() and dv_data is not None:\n",
    "    print(\"üîç Starting AI analysis...\")\n",
    "    \n",
    "    # Choose your AI client (uncomment the one you're using)\n",
    "    # ai_client = azure_client    # For Azure OpenAI\n",
    "    # ai_client = openai_client   # For standard OpenAI\n",
    "    # ai_client = gemini_model    # For Google Gemini\n",
    "    \n",
    "    # For demo purposes, we'll simulate the analysis\n",
    "    print(\"üìù Analysis would be performed here with your chosen AI service\")\n",
    "    print(\"üí° Example insights would include:\")\n",
    "    print(\"   - Bottlenecks in approval process\")\n",
    "    print(\"   - Units with longest processing times\")\n",
    "    print(\"   - Transaction types causing delays\")\n",
    "    print(\"   - Recommendations for improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad72022",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Process and Visualize API Results**\n",
    "\n",
    "### **üìà Create Interactive Dashboards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1db6ae96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No data available for visualization. Please check database connection.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "def create_processing_time_dashboard(df):\n",
    "    \"\"\"Create comprehensive processing time dashboard\"\"\"\n",
    "    \n",
    "    # Create subplot figure\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Processing Time by Status', 'Processing Time by Unit', \n",
    "                       'Monthly Trends', 'Transaction Type Analysis'),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"box\"}],\n",
    "               [{\"type\": \"scatter\"}, {\"type\": \"pie\"}]]\n",
    "    )\n",
    "    \n",
    "    # 1. Processing time by status\n",
    "    status_avg = df.groupby('status')['total_processing_days'].mean().reset_index()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=status_avg['status'], y=status_avg['total_processing_days'],\n",
    "               name='Avg Processing Days', marker_color='lightblue'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Box plot by implementing unit\n",
    "    units = df['implementing_unit'].unique()\n",
    "    for unit in units:\n",
    "        unit_data = df[df['implementing_unit'] == unit]['total_processing_days']\n",
    "        fig.add_trace(\n",
    "            go.Box(y=unit_data, name=unit, boxpoints='outliers'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Monthly trends (simulated)\n",
    "    df['month'] = pd.to_datetime(df['created_at']).dt.to_period('M') if 'created_at' in df.columns else pd.period_range('2025-01', periods=len(df), freq='M')\n",
    "    monthly_avg = df.groupby('month')['total_processing_days'].mean().reset_index()\n",
    "    monthly_avg['month_str'] = monthly_avg['month'].astype(str)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=monthly_avg['month_str'], y=monthly_avg['total_processing_days'],\n",
    "                  mode='lines+markers', name='Monthly Trend', line_color='red'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Transaction type distribution\n",
    "    type_counts = df['transaction_type'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=type_counts.index, values=type_counts.values,\n",
    "               name=\"Transaction Types\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"DV Processing Analytics Dashboard\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def create_bottleneck_heatmap(df):\n",
    "    \"\"\"Create heatmap showing bottlenecks by unit and transaction type\"\"\"\n",
    "    \n",
    "    # Create pivot table for heatmap\n",
    "    pivot_table = df.pivot_table(\n",
    "        values='total_processing_days',\n",
    "        index='implementing_unit',\n",
    "        columns='transaction_type',\n",
    "        aggfunc='mean',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    sns.heatmap(pivot_table, \n",
    "                annot=True, \n",
    "                fmt='.1f', \n",
    "                cmap='RdYlBu_r',\n",
    "                center=pivot_table.mean().mean(),\n",
    "                ax=ax)\n",
    "    \n",
    "    plt.title('Average Processing Days by Unit and Transaction Type')\n",
    "    plt.xlabel('Transaction Type')\n",
    "    plt.ylabel('Implementing Unit')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def generate_insights_summary(analysis_result):\n",
    "    \"\"\"Generate executive summary from AI analysis\"\"\"\n",
    "    \n",
    "    if analysis_result:\n",
    "        print(\"üéØ EXECUTIVE SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(analysis_result[:500] + \"...\" if len(analysis_result) > 500 else analysis_result)\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "    else:\n",
    "        print(\"üìä SAMPLE INSIGHTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. BOTTLENECK ANALYSIS:\")\n",
    "        print(\"   - Approval stage shows longest delays (avg 8.5 days)\")\n",
    "        print(\"   - Review process varies significantly by unit\")\n",
    "        print(\"   - Equipment purchases take 40% longer than services\")\n",
    "        print(\"\\n2. RECOMMENDATIONS:\")\n",
    "        print(\"   - Implement parallel approval workflows\")\n",
    "        print(\"   - Standardize review checklists across units\")\n",
    "        print(\"   - Pre-approve common transaction types\")\n",
    "        print(\"\\n3. PRIORITY ACTIONS:\")\n",
    "        print(\"   - Focus on RAED unit efficiency improvement\")\n",
    "        print(\"   - Automate routine approval processes\")\n",
    "        print(\"   - Set up real-time bottleneck alerts\")\n",
    "\n",
    "def save_results_to_excel(df, analysis_result, filename=\"dv_analysis_results.xlsx\"):\n",
    "    \"\"\"Save analysis results to Excel file\"\"\"\n",
    "    \n",
    "    with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "        # Raw data\n",
    "        df.to_excel(writer, sheet_name='Raw Data', index=False)\n",
    "        \n",
    "        # Summary statistics\n",
    "        summary_stats = pd.DataFrame({\n",
    "            'Metric': ['Total DVs', 'Avg Processing Days', 'Max Processing Days', 'Min Processing Days'],\n",
    "            'Value': [len(df), df['total_processing_days'].mean(), \n",
    "                     df['total_processing_days'].max(), df['total_processing_days'].min()]\n",
    "        })\n",
    "        summary_stats.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Processing by unit\n",
    "        unit_summary = df.groupby('implementing_unit').agg({\n",
    "            'total_processing_days': ['mean', 'median', 'std', 'count']\n",
    "        }).round(2)\n",
    "        unit_summary.to_excel(writer, sheet_name='By Unit')\n",
    "        \n",
    "        # Analysis insights (if available)\n",
    "        if analysis_result:\n",
    "            insights_df = pd.DataFrame({'AI Analysis': [analysis_result]})\n",
    "            insights_df.to_excel(writer, sheet_name='AI Insights', index=False)\n",
    "    \n",
    "    print(f\"‚úÖ Results saved to {filename}\")\n",
    "\n",
    "# Create visualizations if we have data\n",
    "if 'dv_data' in locals() and dv_data is not None:\n",
    "    print(\"üìä Creating visualizations...\")\n",
    "    \n",
    "    # Create dashboard\n",
    "    dashboard = create_processing_time_dashboard(dv_data)\n",
    "    \n",
    "    # Display dashboard (uncomment to show)\n",
    "    # dashboard.show()\n",
    "    \n",
    "    # Create heatmap\n",
    "    heatmap_fig = create_bottleneck_heatmap(dv_data)\n",
    "    \n",
    "    # Display heatmap (uncomment to show)\n",
    "    # plt.show()\n",
    "    \n",
    "    # Generate insights summary\n",
    "    generate_insights_summary(None)  # Using sample insights\n",
    "    \n",
    "    # Save results\n",
    "    save_results_to_excel(dv_data, \"Sample AI analysis results for demonstration\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Analysis complete! Check the generated files for detailed results.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data available for visualization. Please check database connection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7181423a",
   "metadata": {},
   "source": [
    "## üîÑ **Laravel Integration for Real-Time Analytics**\n",
    "\n",
    "### **Step 1: Create AI Analytics Service in Laravel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c975566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a22ec550",
   "metadata": {},
   "source": [
    "### **Step 2: Update React Statistics Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea53660e",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character 'ü§ñ' (U+1F916) (1958211984.py, line 74)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 74\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m<span className=\"mr-3\">ü§ñ</span>\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid character 'ü§ñ' (U+1F916)\n"
     ]
    }
   ],
   "source": [
    "// This cell is intentionally left empty.\n",
    "// If you want to use JavaScript/JSX, please move your code to a .js or .jsx file in your React project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc1d89d",
   "metadata": {},
   "source": [
    "## ‚úÖ **Implementation Checklist**\n",
    "\n",
    "### **Phase 1: Setup (Week 1)**\n",
    "- [ ] Choose AI service (Azure OpenAI recommended for government)\n",
    "- [ ] Register and get API access\n",
    "- [ ] Set up development environment\n",
    "- [ ] Install required packages\n",
    "- [ ] Configure environment variables\n",
    "\n",
    "### **Phase 2: Basic Integration (Week 2)**\n",
    "- [ ] Create Laravel AI service class\n",
    "- [ ] Set up database queries for analytics\n",
    "- [ ] Test API connections\n",
    "- [ ] Create basic analysis functions\n",
    "- [ ] Add error handling\n",
    "\n",
    "### **Phase 3: Frontend Integration (Week 3)**\n",
    "- [ ] Update React statistics page\n",
    "- [ ] Add AI analysis buttons\n",
    "- [ ] Implement loading states\n",
    "- [ ] Create results display components\n",
    "- [ ] Add export functionality\n",
    "\n",
    "### **Phase 4: Advanced Features (Week 4)**\n",
    "- [ ] Real-time analytics\n",
    "- [ ] Automated insights generation\n",
    "- [ ] Email reports scheduling\n",
    "- [ ] Performance monitoring\n",
    "- [ ] User feedback system\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ **Next Steps**\n",
    "\n",
    "1. **Start with Azure OpenAI** - Best for government compliance\n",
    "2. **Begin with simple analysis** - Focus on bottleneck detection first\n",
    "3. **Test with sample data** - Use the provided code examples\n",
    "4. **Gradual rollout** - Start with pilot users\n",
    "5. **Monitor costs** - Set up usage alerts\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Cost Estimation**\n",
    "\n",
    "### **Azure OpenAI Costs (Estimated)**\n",
    "- **GPT-4**: ~$0.03 per 1K tokens (~$3-5 per analysis)\n",
    "- **GPT-3.5**: ~$0.002 per 1K tokens (~$0.20 per analysis)\n",
    "- **Monthly estimate**: $50-200 for regular use\n",
    "\n",
    "### **Alternative Options**\n",
    "- **Google Gemini**: 50% cheaper than OpenAI\n",
    "- **Local AI**: One-time setup cost, no recurring fees\n",
    "- **Claude**: Similar to OpenAI pricing\n",
    "\n",
    "---\n",
    "\n",
    "## üîí **Security Considerations**\n",
    "\n",
    "- **Data Privacy**: Ensure no sensitive data leaves your environment\n",
    "- **API Key Security**: Use environment variables, rotate regularly\n",
    "- **Access Control**: Limit who can trigger AI analysis\n",
    "- **Audit Logging**: Track all AI API calls\n",
    "- **Compliance**: Verify AI service meets government requirements\n",
    "\n",
    "---\n",
    "\n",
    "## üìû **Support Resources**\n",
    "\n",
    "- **Azure OpenAI Documentation**: [docs.microsoft.com/azure/cognitive-services/openai](https://docs.microsoft.com/azure/cognitive-services/openai)\n",
    "- **OpenAI API Docs**: [platform.openai.com/docs](https://platform.openai.com/docs)\n",
    "- **Google AI Studio**: [makersuite.google.com](https://makersuite.google.com)\n",
    "- **Laravel Documentation**: [laravel.com/docs](https://laravel.com/docs)\n",
    "\n",
    "**üéØ Ready to revolutionize your DV processing with AI? Start with Phase 1 today!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
